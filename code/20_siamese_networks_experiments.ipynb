{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import itertools\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob(\"../dataset/images*/*.jpg\")\n",
    "names = list(map(lambda path: path.split('/')[-1], paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dimensions(image_path: str):\n",
    "    try:\n",
    "        im = Image.open(image_path)\n",
    "    except:\n",
    "        return (1, 1)\n",
    "    width, height = im.size\n",
    "    del im\n",
    "    return (width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = list(map(get_dimensions, paths))\n",
    "aspect_ratio = list(map(lambda x: x[0] / x[1], dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = pd.DataFrame({\n",
    "    \"name\": names,\n",
    "    \"dimensions\": dimensions,\n",
    "    \"aspect_ratio\": aspect_ratio\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dimensions\n",
       "(800, 600)     0.421238\n",
       "(600, 800)     0.261531\n",
       "(525, 700)     0.066371\n",
       "(933, 700)     0.058657\n",
       "(1280, 960)    0.013475\n",
       "                 ...   \n",
       "(952, 625)     0.000005\n",
       "(519, 768)     0.000005\n",
       "(1200, 810)    0.000005\n",
       "(828, 317)     0.000005\n",
       "(1400, 908)    0.000005\n",
       "Name: proportion, Length: 4740, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims['dimensions'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aspect_ratio\n",
       "1.333333    0.443474\n",
       "0.750000    0.350599\n",
       "1.332857    0.058662\n",
       "1.500586    0.011182\n",
       "1.000000    0.010357\n",
       "              ...   \n",
       "1.512915    0.000005\n",
       "0.919558    0.000005\n",
       "1.835000    0.000005\n",
       "1.408083    0.000005\n",
       "1.541850    0.000005\n",
       "Name: proportion, Length: 4092, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims['aspect_ratio'].value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning from broken images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../data\")\n",
    "COMP_DATA_PATH = Path(\"../data\")\n",
    "IMAGE_PATH = Path(\"../dataset\")\n",
    "\n",
    "TEST_SET = COMP_DATA_PATH / \"test-data.csv\"\n",
    "TRAIN_SPLIT = DATA_PATH / \"train_split_20perval_grouped_stratified.csv\"\n",
    "VAL_SPLIT = DATA_PATH / \"val_split_20perval_grouped_stratified.csv\"\n",
    "\n",
    "IMG_GLOB = \"images*/*.jpg\"\n",
    "def bind_fs(df, path: Path, glob: str):\n",
    "    mapping = {x.name: x for x in path.glob(glob)}\n",
    "    return df.applymap(lambda x: mapping.get(x))\n",
    "val_df = pd.read_csv(VAL_SPLIT)\n",
    "val_df[[\"image_path1\", \"image_path2\"]] = bind_fs(val_df.filter(like=\"image_url\"), IMAGE_PATH, IMG_GLOB)\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_SPLIT)\n",
    "train_df[[\"image_path1\", \"image_path2\"]] = bind_fs(train_df.filter(like=\"image_url\"), IMAGE_PATH, IMG_GLOB)\n",
    "\n",
    "test_df = pd.read_csv(TEST_SET)\n",
    "test_df[[\"image_url1\", \"image_url2\"]] = test_df[[\"image_url1\", \"image_url2\"]].applymap(lambda x: x.rsplit(\"/\", 1)[-1])\n",
    "test_df[[\"image_path1\", \"image_path2\"]] = bind_fs(test_df.filter(like=\"image_url\"), IMAGE_PATH, IMG_GLOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_broken_imgs(row):\n",
    "    try:\n",
    "        im1 = Image.open(row[\"image_path1\"])\n",
    "        im2 = Image.open(row[\"image_path2\"])\n",
    "        del im1\n",
    "        del im2\n",
    "    except:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_filtered = train_df[train_df.apply(filter_broken_imgs, axis=1)]\n",
    "val_df_filtered = val_df[val_df.apply(filter_broken_imgs, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72487, 71385, 18151, 17872)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(train_df_filtered), len(val_df), len(val_df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_filtered.to_csv(\"../data/train_split_filtered.csv\", index=False)\n",
    "val_df_filtered.to_csv(\"../data/val_split_filtered.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./12-siamese-network/code.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "IMG_SIZE = (512, 512)\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT_FILTERED_PATH = DATA_PATH / \"train_split_filtered.csv\"\n",
    "VAL_SPLIT_FILTERED_PATH = DATA_PATH / \"val_split_filtered.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_filtered = pd.read_csv(TRAIN_SPLIT_FILTERED_PATH)\n",
    "val_split_filtered = pd.read_csv(VAL_SPLIT_FILTERED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE)),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE)),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SiameseNetworkDataset(train_split_filtered, transform=train_transforms)\n",
    "val_dataset = SiameseNetworkDataset(val_split_filtered, transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = SiameseNetworkDataset(train_split_filtered.iloc[:64], transform=train_transforms)\n",
    "# val_dataset = SiameseNetworkDataset(val_split_filtered.iloc[:64], transform=val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[5][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     print(batch[0].shape, batch[1].shape, batch[2].shape)\n",
    "#     break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(11, 11), stride=(1, 1), padding=same)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (13): ReLU()\n",
       "    (14): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (17): ReLU()\n",
       "    (18): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=32768, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc3): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512]) torch.Size([4, 3, 512, 512]) torch.Size([4])\n",
      "torch.Size([4, 1]) torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    img1, img2, label = batch\n",
    "    print(img1.shape, img2.shape, label.shape)\n",
    "    output = net(img1, img2)\n",
    "    print(output[0].shape, output[1].shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikita-fordui\u001b[0m (\u001b[33mcsc_hackathon_lun\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nikiandr/Projects/Fun/lun_hackathon/code/wandb/run-20230703_175803-zbkylykx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/csc_hackathon_lun/csc_hackathon_lun/runs/zbkylykx' target=\"_blank\">northern-durian-54</a></strong> to <a href='https://wandb.ai/csc_hackathon_lun/csc_hackathon_lun' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/csc_hackathon_lun/csc_hackathon_lun' target=\"_blank\">https://wandb.ai/csc_hackathon_lun/csc_hackathon_lun</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/csc_hackathon_lun/csc_hackathon_lun/runs/zbkylykx' target=\"_blank\">https://wandb.ai/csc_hackathon_lun/csc_hackathon_lun/runs/zbkylykx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/csc_hackathon_lun/csc_hackathon_lun/runs/zbkylykx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb1f48f3eb0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"csc_hackathon_lun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiameseNetwork().to(device)\n",
    "loss_fn = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "        # Every data instance is an input + label pair\n",
    "        input1, input2, labels = data\n",
    "\n",
    "        # to gpu\n",
    "        input1 = input1.to(device)\n",
    "        input2 = input2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        output1, output2 = net(input1, input2)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(output1, output2, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % len(train_dataloader) == (len(train_dataloader) - 1):\n",
    "            last_loss = running_loss / (len(train_dataloader) - 1): # loss per batch\n",
    "            tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11d39a1a03a4c8f839779214e5d5b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17847 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikiandr/anaconda3/envs/torch/lib/python3.10/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m net\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(epoch_number)\n\u001b[1;32m     11\u001b[0m running_vloss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# statistics for batch normalization.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index)\u001b[0m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m \u001b[39m# Gather data and report\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39mlen\u001b[39m(train_dataloader) \u001b[39m==\u001b[39m (\u001b[39mlen\u001b[39m(train_dataloader) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     33\u001b[0m     last_loss \u001b[39m=\u001b[39m running_loss \u001b[39m/\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m# loss per batch\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_vloss = 1_000_000.\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "for epoch_number in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    net.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    net.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_dataloader):\n",
    "            vinput1, vinput2, vlabels = vdata\n",
    "            # to gpu\n",
    "            vinput1 = vinput1.to(device)\n",
    "            vinput2 = vinput2.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "            voutput1, voutput2 = net(vinput1, vinput2)\n",
    "            vloss = loss_fn(voutput1, voutput2, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
